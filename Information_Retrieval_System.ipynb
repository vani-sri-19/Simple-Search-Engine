{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "import xml.etree.ElementTree as ET\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# Initialize stopword list and lemmatizer\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "STEMMER = PorterStemmer()\n",
        "\n",
        "# Dataset URLs from GitHub\n",
        "BASE_URL = \"https://raw.githubusercontent.com/oussbenk/cranfield-trec-dataset/main/\"\n",
        "FILES = {\n",
        "    \"documents\": \"cran.all.1400.xml\",\n",
        "    \"queries\": \"cran.qry.xml\",\n",
        "    \"treceval\": \"cranqrel.trec.txt\"\n",
        "}\n",
        "\n",
        "# Download dataset files from GitHub\n",
        "def download_file(filename):\n",
        "    url = BASE_URL + filename\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"Failed to download: {filename}\")\n",
        "\n",
        "for key, filename in FILES.items():\n",
        "    download_file(filename)\n",
        "\n",
        "# File paths\n",
        "DOCUMENTS_FILE = FILES[\"documents\"]\n",
        "QUERIES_FILE = FILES[\"queries\"]\n",
        "RELEVANCE_FILE = FILES[\"treceval\"]\n",
        "OUTPUT_FILE_VSM = \"output_trec_vsm.txt\"\n",
        "OUTPUT_FILE_BM = \"output_trec_bm.txt\"\n",
        "OUTPUT_FILE_LM = \"output_trec_lm.txt\"\n",
        "\n",
        "# Data structures\n",
        "documents = {}\n",
        "queries = {}\n",
        "inverted_index = collections.defaultdict(dict)\n",
        "doc_lengths = {}\n",
        "\n",
        "# Text Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lower Case\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Remove special characters\n",
        "    text = re.sub(r\"<[^>]+>\", \"\", text) # Remove HTML/XML Tags\n",
        "    text = re.sub(r\"\\d+\", \"\", text) # Remove Numbers\n",
        "    words = word_tokenize(text) # Tokenization (Split text into words)\n",
        "    words = [LEMMATIZER.lemmatize(word) for word in words if word not in STOPWORDS] # Remove Stopwords & Lemmatization\n",
        "    words = [STEMMER.stem(word) for word in words]  # Stemming\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "# Parse Documents\n",
        "def parse_documents():\n",
        "    with open(DOCUMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read()\n",
        "\n",
        "    docs = re.findall(r\"<doc>(.*?)</doc>\", raw_text, re.DOTALL)\n",
        "\n",
        "    for doc in docs:\n",
        "        doc_id = re.search(r\"<docno>\\s*(\\d+)\\s*</docno>\", doc)\n",
        "        text = re.search(r\"<text>(.*?)</text>\", doc, re.DOTALL)\n",
        "\n",
        "        if doc_id and text:\n",
        "            doc_id = int(doc_id.group(1))\n",
        "            text = preprocess_text(text.group(1))  # Apply preprocessing\n",
        "            documents[doc_id] = text\n",
        "\n",
        "    print(f\"Parsed {len(documents)} documents.\")\n",
        "\n",
        "# Build Inverted Index\n",
        "def build_inverted_index():\n",
        "    print(\"Building inverted index...\")\n",
        "    global doc_lengths\n",
        "    for doc_id, text in documents.items():\n",
        "        words = text.split()\n",
        "        doc_lengths[doc_id] = len(words)\n",
        "        for word in words:\n",
        "            inverted_index[word][doc_id] = inverted_index[word].get(doc_id, 0) + 1\n",
        "    \"\"\"print(\"\\nInverted Index:\")\n",
        "    for word, doc_dict in inverted_index.items():\n",
        "        print(f\"'{word}': {dict(doc_dict)}\")  \"\"\"\n",
        "\n",
        "# Parse Queries\n",
        "def parse_queries():\n",
        "    tree = ET.parse(QUERIES_FILE)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for query in root.findall(\"top\"):\n",
        "        query_id = int(query.find(\"num\").text.strip())\n",
        "        text = query.find(\"title\").text.strip()\n",
        "        queries[query_id] = preprocess_text(text)  # Apply preprocessing\n",
        "\n",
        "    print(f\"Parsed {len(queries)} queries.\")\n",
        "\n",
        "# Run Pipeline\n",
        "parse_documents()\n",
        "build_inverted_index()\n",
        "parse_queries()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "066b5e3c-52be-4594-dfb2-f9cc98230958",
        "id": "3weMAO68CaTB"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: cran.all.1400.xml\n",
            "Downloaded: cran.qry.xml\n",
            "Downloaded: cranqrel.trec.txt\n",
            "Parsed 1400 documents.\n",
            "Building inverted index...\n",
            "Parsed 225 queries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and compile trec_eval\n",
        "!wget -q https://github.com/usnistgov/trec_eval/archive/refs/tags/v9.0.7.tar.gz\n",
        "!tar -xzf v9.0.7.tar.gz\n",
        "!cd trec_eval-9.0.7 && make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QmcwqPY8H0D",
        "outputId": "d53ccb4c-1224-4cd0-c0f2-0019ede2897e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc -g -I.  -Wall -DVERSIONID=\\\"9.0.7\\\"  -o trec_eval trec_eval.c formats.c meas_init.c meas_acc.c meas_avg.c meas_print_single.c meas_print_final.c get_qrels.c get_trec_results.c get_prefs.c get_qrels_prefs.c get_qrels_jg.c form_res_rels.c form_res_rels_jg.c form_prefs_counts.c utility_pool.c get_zscores.c convert_zscores.c measures.c  m_map.c m_P.c m_num_q.c m_num_ret.c m_num_rel.c m_num_rel_ret.c m_gm_map.c m_Rprec.c m_recip_rank.c m_bpref.c m_iprec_at_recall.c m_recall.c m_Rprec_mult.c m_utility.c m_11pt_avg.c m_ndcg.c m_ndcg_cut.c m_Rndcg.c m_ndcg_rel.c m_binG.c m_G.c m_rel_P.c m_success.c m_infap.c m_map_cut.c m_gm_bpref.c m_runid.c m_relstring.c m_set_P.c m_set_recall.c m_set_rel_P.c m_set_map.c m_set_F.c m_num_nonrel_judged_ret.c m_prefs_num_prefs_poss.c m_prefs_num_prefs_ful.c m_prefs_num_prefs_ful_ret.c m_prefs_simp.c m_prefs_pair.c m_prefs_avgjg.c m_prefs_avgjg_Rnonrel.c m_prefs_simp_ret.c m_prefs_pair_ret.c m_prefs_avgjg_ret.c m_prefs_avgjg_Rnonrel_ret.c m_prefs_simp_imp.c m_prefs_pair_imp.c m_prefs_avgjg_imp.c m_map_avgjg.c m_Rprec_mult_avgjg.c m_P_avgjg.c m_yaap.c -lm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "\n",
        "# Compute TF-IDF (Vector Space Model)\n",
        "def compute_vsm(query):\n",
        "    scores = collections.defaultdict(float)\n",
        "    words = query.split()\n",
        "    for word in words:\n",
        "        if word in inverted_index:\n",
        "            df = len(inverted_index[word])\n",
        "            idf = math.log(len(documents) / df)\n",
        "            for doc_id, tf in inverted_index[word].items():\n",
        "                scores[doc_id] += tf * idf\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Run retrieval and write results\n",
        "def run_retrieval():\n",
        "    with open(OUTPUT_FILE_VSM, \"w\") as f:\n",
        "        new_query_id = 1  # Start numbering from 1\n",
        "        for original_query_id in sorted(queries.keys()):\n",
        "            query_text = queries[original_query_id]\n",
        "            vsm_scores = compute_vsm(query_text)[:10]\n",
        "            write_results(f, new_query_id, \"VSM\", vsm_scores)\n",
        "            new_query_id += 1\n",
        "\n",
        "# Write results in TREC format\n",
        "def write_results(f, query_id, run_id, scores):\n",
        "    rank = 1\n",
        "    for doc_id, score in scores:\n",
        "        score = 1 if score > 0 else 0  # Convert scores to binary (1 or 0)\n",
        "        f.write(f\"{query_id} 0 {doc_id} {score} 1 {run_id}\\n\")\n",
        "        rank += 1\n",
        "\n",
        "run_retrieval()\n",
        "\n",
        "# Check output\n",
        "!head -n 20 output_trec_vsm.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4772eb-63a7-499b-fe15-1f73ecb4c6a8",
        "id": "PeHjgzPxZoSk"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0 51 1 1 VSM\n",
            "1 0 874 1 1 VSM\n",
            "1 0 486 1 1 VSM\n",
            "1 0 792 1 1 VSM\n",
            "1 0 329 1 1 VSM\n",
            "1 0 1144 1 1 VSM\n",
            "1 0 56 1 1 VSM\n",
            "1 0 1328 1 1 VSM\n",
            "1 0 252 1 1 VSM\n",
            "1 0 359 1 1 VSM\n",
            "2 0 51 1 1 VSM\n",
            "2 0 12 1 1 VSM\n",
            "2 0 792 1 1 VSM\n",
            "2 0 100 1 1 VSM\n",
            "2 0 1147 1 1 VSM\n",
            "2 0 1169 1 1 VSM\n",
            "2 0 640 1 1 VSM\n",
            "2 0 746 1 1 VSM\n",
            "2 0 14 1 1 VSM\n",
            "2 0 712 1 1 VSM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run trec_eval using the relevance file\n",
        "!trec_eval-9.0.7/trec_eval cranqrel.trec.txt output_trec_vsm.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxOddh1b8d7c",
        "outputId": "43822750-aae3-40cd-a50a-02d5c5c32633"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runid                 \tall\tVSM\n",
            "num_q                 \tall\t225\n",
            "num_ret               \tall\t2250\n",
            "num_rel               \tall\t1612\n",
            "num_rel_ret           \tall\t378\n",
            "map                   \tall\t0.1202\n",
            "gm_map                \tall\t0.0102\n",
            "Rprec                 \tall\t0.1690\n",
            "bpref                 \tall\t0.2283\n",
            "recip_rank            \tall\t0.2938\n",
            "iprec_at_recall_0.00  \tall\t0.3498\n",
            "iprec_at_recall_0.10  \tall\t0.3312\n",
            "iprec_at_recall_0.20  \tall\t0.2657\n",
            "iprec_at_recall_0.30  \tall\t0.1948\n",
            "iprec_at_recall_0.40  \tall\t0.1359\n",
            "iprec_at_recall_0.50  \tall\t0.1178\n",
            "iprec_at_recall_0.60  \tall\t0.0615\n",
            "iprec_at_recall_0.70  \tall\t0.0460\n",
            "iprec_at_recall_0.80  \tall\t0.0264\n",
            "iprec_at_recall_0.90  \tall\t0.0264\n",
            "iprec_at_recall_1.00  \tall\t0.0264\n",
            "P_5                   \tall\t0.1822\n",
            "P_10                  \tall\t0.1680\n",
            "P_15                  \tall\t0.1120\n",
            "P_20                  \tall\t0.0840\n",
            "P_30                  \tall\t0.0560\n",
            "P_100                 \tall\t0.0168\n",
            "P_200                 \tall\t0.0084\n",
            "P_500                 \tall\t0.0034\n",
            "P_1000                \tall\t0.0017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "\n",
        "# Compute BM25\n",
        "def compute_bm25(query, k1=1.5, b=0.75):\n",
        "    scores = collections.defaultdict(float)\n",
        "    words = query.split()\n",
        "    avg_doc_length = sum(doc_lengths.values()) / len(doc_lengths)\n",
        "\n",
        "    for word in words:\n",
        "        if word in inverted_index:\n",
        "            df = len(inverted_index[word])\n",
        "            idf = math.log((len(documents) - df + 0.5) / (df + 0.5))\n",
        "            for doc_id, tf in inverted_index[word].items():\n",
        "                norm_tf = tf * (k1 + 1) / (tf + k1 * (1 - b + b * doc_lengths[doc_id] / avg_doc_length))\n",
        "                scores[doc_id] += idf * norm_tf\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Run retrieval and write results\n",
        "def run_retrieval():\n",
        "    with open(OUTPUT_FILE_BM, \"w\") as f:\n",
        "        new_query_id = 1\n",
        "        for original_query_id in sorted(queries.keys()):\n",
        "            query_text = queries[original_query_id]\n",
        "            bm25_scores = compute_bm25(query_text)[:20]\n",
        "            write_results(f, new_query_id, \"BM25\", bm25_scores)\n",
        "            new_query_id += 1\n",
        "\n",
        "# Write results in TREC format\n",
        "def write_results(f, query_id, run_id, scores):\n",
        "    rank = 1\n",
        "    for doc_id, score in scores:\n",
        "        score = 1 if score > 0 else 0  # Convert scores to binary (1 or 0)\n",
        "        f.write(f\"{query_id} 0 {doc_id} {rank} {score} {run_id}\\n\")\n",
        "        rank += 1\n",
        "\n",
        "run_retrieval()\n",
        "\n",
        "# Check output\n",
        "!head -n 20 output_trec_bm.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8js2EkAXwF8G",
        "outputId": "a4db59db-b941-4874-a413-ab3627a8b533"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0 51 1 1 BM25\n",
            "1 0 486 2 1 BM25\n",
            "1 0 878 3 1 BM25\n",
            "1 0 573 4 1 BM25\n",
            "1 0 12 5 1 BM25\n",
            "1 0 746 6 1 BM25\n",
            "1 0 944 7 1 BM25\n",
            "1 0 13 8 1 BM25\n",
            "1 0 879 9 1 BM25\n",
            "1 0 141 10 1 BM25\n",
            "1 0 747 11 1 BM25\n",
            "1 0 1361 12 1 BM25\n",
            "1 0 172 13 1 BM25\n",
            "1 0 359 14 1 BM25\n",
            "1 0 665 15 1 BM25\n",
            "1 0 663 16 1 BM25\n",
            "1 0 78 17 1 BM25\n",
            "1 0 1003 18 1 BM25\n",
            "1 0 219 19 1 BM25\n",
            "1 0 184 20 1 BM25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run trec_eval using the relevance file\n",
        "!trec_eval-9.0.7/trec_eval cranqrel.trec.txt output_trec_bm.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlWFxT_rEx6w",
        "outputId": "ff9bf309-02c9-4063-932c-4b67c99e52ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runid                 \tall\tBM25\n",
            "num_q                 \tall\t225\n",
            "num_ret               \tall\t4500\n",
            "num_rel               \tall\t1612\n",
            "num_rel_ret           \tall\t689\n",
            "map                   \tall\t0.1615\n",
            "gm_map                \tall\t0.0458\n",
            "Rprec                 \tall\t0.1640\n",
            "bpref                 \tall\t0.3062\n",
            "recip_rank            \tall\t0.2844\n",
            "iprec_at_recall_0.00  \tall\t0.3647\n",
            "iprec_at_recall_0.10  \tall\t0.3479\n",
            "iprec_at_recall_0.20  \tall\t0.3066\n",
            "iprec_at_recall_0.30  \tall\t0.2501\n",
            "iprec_at_recall_0.40  \tall\t0.2246\n",
            "iprec_at_recall_0.50  \tall\t0.1946\n",
            "iprec_at_recall_0.60  \tall\t0.1371\n",
            "iprec_at_recall_0.70  \tall\t0.1117\n",
            "iprec_at_recall_0.80  \tall\t0.0711\n",
            "iprec_at_recall_0.90  \tall\t0.0453\n",
            "iprec_at_recall_1.00  \tall\t0.0453\n",
            "P_5                   \tall\t0.1742\n",
            "P_10                  \tall\t0.1787\n",
            "P_15                  \tall\t0.1689\n",
            "P_20                  \tall\t0.1531\n",
            "P_30                  \tall\t0.1021\n",
            "P_100                 \tall\t0.0306\n",
            "P_200                 \tall\t0.0153\n",
            "P_500                 \tall\t0.0061\n",
            "P_1000                \tall\t0.0031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "\n",
        "# Compute Language Model (Jelinek-Mercer Smoothing)\n",
        "def compute_language_model(query, lambda_=0.2):\n",
        "    scores = {}\n",
        "    words = query.split()\n",
        "    collection_size = sum(doc_lengths.values())\n",
        "\n",
        "    for doc_id, text in documents.items():\n",
        "        doc_words = text.split()\n",
        "        doc_len = len(doc_words)\n",
        "        score = 1.0\n",
        "        for word in words:\n",
        "            p_doc = doc_words.count(word) / doc_len if doc_len > 0 else 0\n",
        "            p_coll = sum(inverted_index[word].values()) / collection_size if word in inverted_index else 0\n",
        "            score *= (1 - lambda_) * p_doc + lambda_ * p_coll\n",
        "        scores[doc_id] = score\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Run retrieval and write results\n",
        "def run_retrieval():\n",
        "    with open(OUTPUT_FILE_LM, \"w\") as f:\n",
        "        new_query_id = 1\n",
        "        for original_query_id in sorted(queries.keys()):\n",
        "            query_text = queries[original_query_id]\n",
        "            lm_scores = compute_language_model(query_text)[:10]\n",
        "            write_results(f, new_query_id, \"LM\", lm_scores)\n",
        "            new_query_id += 1\n",
        "\n",
        "# Write results in TREC format\n",
        "def write_results(f, query_id, run_id, scores):\n",
        "    rank = 1\n",
        "    for doc_id, score in scores:\n",
        "        score = 1 if score > 0 else 0  # Convert scores to binary (1 or 0)\n",
        "        f.write(f\"{query_id} 0 {doc_id} {rank} {score} {run_id}\\n\")\n",
        "        rank += 1\n",
        "\n",
        "run_retrieval()\n",
        "\n",
        "# Check output\n",
        "!head -n 20 output_trec_lm.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DQ8pmYmw_RN",
        "outputId": "ce8151fd-67c0-4635-95cc-b1d7987391a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0 51 1 1 LM\n",
            "1 0 486 2 1 LM\n",
            "1 0 573 3 1 LM\n",
            "1 0 878 4 1 LM\n",
            "1 0 12 5 1 LM\n",
            "1 0 329 6 1 LM\n",
            "1 0 141 7 1 LM\n",
            "1 0 879 8 1 LM\n",
            "1 0 746 9 1 LM\n",
            "1 0 944 10 1 LM\n",
            "2 0 12 1 1 LM\n",
            "2 0 746 2 1 LM\n",
            "2 0 172 3 1 LM\n",
            "2 0 51 4 1 LM\n",
            "2 0 1089 5 1 LM\n",
            "2 0 1380 6 1 LM\n",
            "2 0 14 7 1 LM\n",
            "2 0 700 8 1 LM\n",
            "2 0 810 9 1 LM\n",
            "2 0 1169 10 1 LM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run trec_eval using the relevance file\n",
        "!trec_eval-9.0.7/trec_eval cranqrel.trec.txt output_trec_lm.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYZsOf7rE0qp",
        "outputId": "98e3f199-81bf-4ab0-fb98-8dd010ce97c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "runid                 \tall\tLM\n",
            "num_q                 \tall\t225\n",
            "num_ret               \tall\t2250\n",
            "num_rel               \tall\t1612\n",
            "num_rel_ret           \tall\t379\n",
            "map                   \tall\t0.1252\n",
            "gm_map                \tall\t0.0059\n",
            "Rprec                 \tall\t0.1625\n",
            "bpref                 \tall\t0.1873\n",
            "recip_rank            \tall\t0.3057\n",
            "iprec_at_recall_0.00  \tall\t0.3500\n",
            "iprec_at_recall_0.10  \tall\t0.3222\n",
            "iprec_at_recall_0.20  \tall\t0.2671\n",
            "iprec_at_recall_0.30  \tall\t0.1884\n",
            "iprec_at_recall_0.40  \tall\t0.1441\n",
            "iprec_at_recall_0.50  \tall\t0.1121\n",
            "iprec_at_recall_0.60  \tall\t0.0818\n",
            "iprec_at_recall_0.70  \tall\t0.0558\n",
            "iprec_at_recall_0.80  \tall\t0.0367\n",
            "iprec_at_recall_0.90  \tall\t0.0290\n",
            "iprec_at_recall_1.00  \tall\t0.0290\n",
            "P_5                   \tall\t0.1831\n",
            "P_10                  \tall\t0.1684\n",
            "P_15                  \tall\t0.1123\n",
            "P_20                  \tall\t0.0842\n",
            "P_30                  \tall\t0.0561\n",
            "P_100                 \tall\t0.0168\n",
            "P_200                 \tall\t0.0084\n",
            "P_500                 \tall\t0.0034\n",
            "P_1000                \tall\t0.0017\n"
          ]
        }
      ]
    }
  ]
}